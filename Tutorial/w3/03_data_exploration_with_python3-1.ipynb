{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration with Python\n",
    "\n",
    "## EXERCISE 1: Reading and accessing data\n",
    "\n",
    "### Read the survey response data\n",
    "\n",
    "The `csv` module supports reading and writing of files in comma-separated values (CSV) and similar formats. We use `DictReader` since the first row of our survey responses file is a header. This produces a list of dictionaries, one dictionary per each individual survey response. \n",
    "\n",
    "A _dictionary_ is a data structure in Python that can hold key-value pairs, where we can lookup values by their key (typically a string, cf. Grok module 9). \n",
    "\n",
    "The `pprint` command below prints the dictionary corresponding the the first response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "data = list(csv.DictReader(open('Survey INFO3406 2018s2  - Form Responses 1.csv')))\n",
    "pprint.pprint(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define constants for dictionary keys\n",
    "\n",
    "Before moving on, let's define constants for the keys of this dictionary that will make it a bit easier to use. In our case, the keys are simply the questions from our survey in Week 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = 'Timestamp'\n",
    "BACKGROUND_INDUSTRY = 'What main industry have you worked in?'\n",
    "BACKGROUND_YEARS_PROFESSIONAL = 'How many years professional experience do you have?'\n",
    "BACKGROUND_YEARS_PROGRAMMING = 'How many years programming experience do you have?'\n",
    "BACKGROUND_SKILLS = 'What key experience do you have?'\n",
    "IMPORT_DATA_MANAGEMENT = 'Data management'\n",
    "IMPORT_STATISTICS = 'Statistics'\n",
    "IMPORT_VISUALISATION = 'Visualisation'\n",
    "IMPORT_MACHINE_LEARNING = 'Machine Learning & Data Mining'\n",
    "IMPORT_SOFTWARE_ENGINEERING = 'Software Engineering'\n",
    "IMPORT_COMMUNICATION = 'Communication'\n",
    "GOALS_DEFINITION = 'How would you define Data Analytics in one sentence?'\n",
    "GOALS_SKILLS = 'What key skills do you want to learn?'\n",
    "GOALS_ROLE = 'What kind of role would you like to go into?'\n",
    "GOALS_INDUSTRY = 'What industry would you like to go into?'\n",
    "IMPORT_AREAS = [\n",
    "    IMPORT_DATA_MANAGEMENT,\n",
    "    IMPORT_STATISTICS,\n",
    "    IMPORT_VISUALISATION,\n",
    "    IMPORT_MACHINE_LEARNING,\n",
    "    IMPORT_SOFTWARE_ENGINEERING,\n",
    "    IMPORT_COMMUNICATION\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing data values\n",
    "\n",
    "This allows us to access cells in a row using the column name as a key. For example, the following prints the number of years professional experience for the first respondent. Note that the csv module reads all values as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = data[0]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row 0 corresponds to first respondent since arrays are 0-indexed\n",
    "print(\"response:\", row[BACKGROUND_YEARS_PROFESSIONAL]) # years of professional experience\n",
    "print(\"type:\", type(row[BACKGROUND_YEARS_PROFESSIONAL])) # csv \n",
    "print(\"response (converted):\", float(row[BACKGROUND_YEARS_PROFESSIONAL])) # years of professional experience\n",
    "print(\"type:\", type(float(row[BACKGROUND_YEARS_PROFESSIONAL]))) # convert to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: What is the third respondent's rating for communication as integer value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2: Frequency distribution and mode\n",
    "\n",
    "### Counting data\n",
    "\n",
    "`Counter` from the `collections` module is useful for quickly calculating frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "for row in data:\n",
    "    counts[row[IMPORT_COMMUNICATION]] += 1\n",
    "\n",
    "print(\"Distribution of communication importance ratings:\")\n",
    "for k, v in sorted(counts.items()):\n",
    "    print('{}: {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Calculate distribution of background and goal industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the mode\n",
    "\n",
    "We can also use `Counter` to calculate the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_counter = Counter()\n",
    "for row in data:\n",
    "    mode_counter[row[IMPORT_COMMUNICATION]] += 1\n",
    "print(\"Communication mode:\", mode_counter.most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably will need to calculate the _mode_ more than once. It hence is a good idea to define this as our a local function which we can later call with different parameters, depending on which data we would like to calculate the mode for, and without the need to repeat all its code again and again.\n",
    "\n",
    "In Python, one defines a local function with the **def** statement, followed by the function name and a list of arguments with which we can invoke a function later.\n",
    "\n",
    "Our own 'mode' function is introduced and used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a new 'mode' function\n",
    "def mode(data, column_key):\n",
    "    mode_counter = Counter()\n",
    "    for row in data:\n",
    "        mode_counter[row[column_key]] += 1\n",
    "    return mode_counter.most_common(1)[0][0]\n",
    "\n",
    "# example on how to use the 'mode' function\n",
    "print(\"Communication mode:\", mode(data, IMPORT_COMMUNICATION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Calculate the mode of background and goal industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 3: Calculating descriptive statistics\n",
    "\n",
    "### Cleaning float data\n",
    "\n",
    "Which columns contained ratio data? We need to convert these to numeric types. Let's define another function since we have two ratio variables. Here we replace values that can't be converted with NaN (not a number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "DEFAULT_VALUE = np.nan\n",
    "\n",
    "def clean(data, column_key, convert_function, default_value):\n",
    "    special_values= {'1 year' : 1.0, '2years' : 2.0, '2 years': 2.0, 'Ten' : 10, 'Half a year': 0.5, '6 months': 0.5, '6MONTHS': 0.5, '3 Months': 0.25, '1 month': 1/12}\n",
    "    for row in data:\n",
    "        old_value = row[column_key]\n",
    "        new_value = default_value\n",
    "        try:\n",
    "            if old_value in special_values.keys():\n",
    "                new_value = special_values[old_value]\n",
    "            else:\n",
    "                new_value = convert_function(old_value)\n",
    "        except (ValueError, TypeError):\n",
    "            print('Replacing {} with {} in column {}'.format(row[column_key], new_value, column_key))\n",
    "        row[column_key] = new_value\n",
    "\n",
    "clean(data, BACKGROUND_YEARS_PROFESSIONAL, float, DEFAULT_VALUE)\n",
    "clean(data, BACKGROUND_YEARS_PROGRAMMING,  float, DEFAULT_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning timestamp data\n",
    "\n",
    "We may also want to convert timestamp values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][TIMESTAMP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "FMT = \"%m/%d/%Y %H:%M:%S\"\n",
    "\n",
    "def str_to_time(s):\n",
    "    if isinstance(s, datetime):\n",
    "        return s\n",
    "    return datetime.strptime(s, FMT)\n",
    "\n",
    "clean(data, TIMESTAMP, str_to_time, DEFAULT_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][TIMESTAMP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics with `numpy`\n",
    "\n",
    "Once the data is converted, we can calculate descriptive statistics. `numpy` includes routines for measures of centrality and dispersion. Below we calculate descriptive statistics for professional and programming experience.\n",
    "\n",
    "Further detail: http://docs.scipy.org/doc/numpy/reference/routines.statistics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for column_key in [BACKGROUND_YEARS_PROFESSIONAL, BACKGROUND_YEARS_PROGRAMMING]:\n",
    "    v = [row[column_key] for row in data] # grab values\n",
    "    print(column_key.upper())\n",
    "    print(\"* Min..Max: {}..{}\".format(np.nanmin(v), np.nanmax(v)))\n",
    "    print(\"* Range: {}\".format(np.nanmax(v)-np.nanmin(v)))\n",
    "    print(\"* Mean: {}\".format(np.nanmean(v)))\n",
    "    print(\"* Standard deviation: {}\".format(np.nanstd(v)))\n",
    "    print(\"* Median: {}\".format(np.nanmedian(v)))\n",
    "    q1 = np.nanpercentile(v, 25)\n",
    "    print(\"* 25th percentile (Q1): {}\".format(q1))\n",
    "    q3 = np.nanpercentile(v, 75)\n",
    "    print(\"* 75th percentile (Q3): {}\".format(q3))\n",
    "    iqr = q3-q1\n",
    "    print(\"* IQR: {}\".format(iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning and histograms\n",
    "\n",
    "`numpy` also provides routines for binning and producing histograms from ratio data.\n",
    "\n",
    "NOTE RuntimeWarning due to NaN values, which are then ignored in histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = []\n",
    "for row in data:\n",
    "    v.append(row[BACKGROUND_YEARS_PROFESSIONAL])\n",
    "\n",
    "freqs, bins = np.histogram(v, bins=7, range=(0,35)) # calculate frequencies and bin start/end\n",
    "for i, freq in enumerate(freqs):\n",
    "    # Note that bins[i] <= bin_values < bins[i+1]\n",
    "    bin_str = '[{}..{}]'.format(int(bins[i]), int(bins[i+1]))\n",
    "    print(bin_str, ':', freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Calculate histogram for programming experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA Calculate histograms with bin size of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## EXERCISE 4: Visualisation with matplotlib\n",
    "\n",
    "### Making a bar chart\n",
    "\n",
    "`matplotlib` provides functionality for creating various plots. Let's start with a frequency polygon. Note the line `%matplotlib inline` in is important in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "IMPORT_KEYS = ['1', '2', '3', '4', '5']\n",
    "def make_importance_plot(data, column_key, title):\n",
    "    c = Counter(row[column_key] for row in data)\n",
    "    d = OrderedDict([(k,c[k]) if k in c else (k,0) for k in IMPORT_KEYS])\n",
    "    # bars are by default width 0.8, so we'll add 0.1 to the left coordinates\n",
    "    xs = [i+0.1 for i,_ in enumerate(IMPORT_KEYS)]\n",
    "    plt.bar(xs, d.values())\n",
    "    plt.ylabel('Number of responses')\n",
    "    plt.axis([0,5,0,50])\n",
    "    plt.title(title)\n",
    "    plt.xticks([i + 0.5 for i, _ in enumerate(IMPORT_KEYS)], IMPORT_KEYS)\n",
    "    plt.show()\n",
    "for a in IMPORT_AREAS:\n",
    "    title = 'Importance of {}'.format(a.lower())\n",
    "    make_importance_plot(data, a, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Make bar charts of known and future industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a histogram\n",
    "\n",
    "Now let's use the `histogram` from `numpy` above to create a histograms of professional and programming experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_histogram(data, column_key):\n",
    "    v = [row[column_key] for row in data] # grab values\n",
    "    freqs, bins = np.histogram(v, bins=7, range=(0,30))\n",
    "    for i, freq in enumerate(freqs):\n",
    "        yield ('[{}..{})'.format(int(bins[i]), int(bins[i+1])), freq)\n",
    "        \n",
    "def make_histogram_plot(data, column_key, title):\n",
    "    d = OrderedDict(iter_histogram(data, column_key))\n",
    "    keys = list(d.keys())\n",
    "    xs = [i+0.1 for i,_ in enumerate(keys)]\n",
    "    plt.bar(xs, d.values())\n",
    "    plt.ylabel('Number of responses')\n",
    "    plt.axis([0,7,0,35])\n",
    "    plt.title(title)\n",
    "    plt.xticks([i + 0.5 for i, _ in enumerate(keys)], keys)\n",
    "    plt.show()\n",
    "    \n",
    "make_histogram_plot(data, BACKGROUND_YEARS_PROFESSIONAL, 'Professional experience')\n",
    "make_histogram_plot(data, BACKGROUND_YEARS_PROGRAMMING, 'Programming experience')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a scatterplot\n",
    "\n",
    "Finally, let's make a scatterplot to compare professional and programming experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "professional_experience = [row[BACKGROUND_YEARS_PROFESSIONAL] for row in data]\n",
    "programming_experience = [row[BACKGROUND_YEARS_PROGRAMMING] for row in data]\n",
    "plt.scatter(professional_experience, programming_experience)\n",
    "plt.title('Professional vs programming experience')\n",
    "plt.xlabel('Years of professional experience')\n",
    "plt.ylabel('Years of programming experience')\n",
    "plt.axis([-1,25,-1,25])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 5: Box plots and correlation\n",
    "\n",
    "### Visualising distributions with box plots\n",
    "\n",
    "Mean and standard deviation are not informative for skewed data. `boxplot` is is a good visualisation for viewing and comparing distributions. It also shows outliers, e.g., values greater than `Q3+1.5*IQR` or less than `Q1-1.5*IQR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "professional_experience = [v for v in professional_experience if v is not np.nan]\n",
    "programming_experience = [v for v in programming_experience if v is not np.nan]\n",
    "plt.boxplot([professional_experience, programming_experience], \n",
    "            vert=False, notch=True, flierprops={'marker':'.'})\n",
    "plt.axis([-1,35,0,3])\n",
    "plt.yticks([1,2], ['Professional', 'Programming'])\n",
    "plt.xlabel('Years experience')\n",
    "plt.title('Professional vs programming experience')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating correlation between two variables\n",
    "\n",
    "Pearson's r is the covariance of the two variables divided by the product of their standard deviations. Spearman rho is a common nonparametric test that is used in stead of Pearson's r when "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# only keep rows where both professional and programming experience are defined\n",
    "prof, prog = [], []\n",
    "for row in data:\n",
    "    if row[BACKGROUND_YEARS_PROFESSIONAL] is np.nan:\n",
    "        continue # ignore rows with no value for professional experience\n",
    "    elif row[BACKGROUND_YEARS_PROGRAMMING] is np.nan:\n",
    "        continue # ignore rows with no value for programming experience\n",
    "    else:\n",
    "        prof.append(row[BACKGROUND_YEARS_PROFESSIONAL])\n",
    "        prog.append(row[BACKGROUND_YEARS_PROGRAMMING])\n",
    "print(\"Pearson (r, p): {}\".format(stats.pearsonr(prof, prog)))\n",
    "\n",
    "print(stats.spearmanr(prof, prog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Kendall's tau between importance ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('{:.2f}'.format(3.14159))\n",
    "for i,k1 in enumerate(IMPORT_AREAS):\n",
    "    print(k1)\n",
    "    v1 = [r[k1] for r in data]\n",
    "    for j in range(i+1,len(IMPORT_AREAS),1):\n",
    "        k2 = IMPORT_AREAS[j]\n",
    "        v2 = [r[k2] for r in data]\n",
    "        tau = stats.kendalltau(v1, v2)\n",
    "        print('* {:5.2f} (p={:.3f}): {}'.format(tau.correlation, tau.pvalue, k2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 6: Text data\n",
    "\n",
    "### Simple tokenisation and word counts\n",
    "\n",
    "Tokenisation is the process of breaking text into it's component parts, e.g., sentences, words. Below is a simple whitespace tokeniser that also removes some leading/trailing punctuation. We can use this to count the frequency of terms acros our data analytics definitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenise(text):\n",
    "    for word in text.lower().split():\n",
    "        yield word.strip('.,')\n",
    "\n",
    "def is_valid_word(w):\n",
    "    if w == '':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def iter_ds_def_words(d):\n",
    "    for row in d:\n",
    "        for word in tokenise(row[GOALS_DEFINITION]):\n",
    "            if is_valid_word(word):\n",
    "                yield word\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter(iter_ds_def_words(data))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "\n",
    "Very common function words can be removed to focus our analysis on content words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = frozenset([ # http://www.nltk.org/book/ch02.html#stopwords_index_term\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "    'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "    ])\n",
    "def is_valid_word(w):\n",
    "    if w == '':\n",
    "        return False\n",
    "    if w.lower() in STOP_WORDS:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "c = Counter(iter_ds_def_words(data))\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting term frequencies\n",
    "\n",
    "Now we can build a simple horizontal bar chart that displays the most common terms across data analytics definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def iter_word_freqs(d, min_freq=4):\n",
    "    c = Counter(iter_ds_def_words(d))\n",
    "    for term, freq in c.items():\n",
    "        if freq >= min_freq:\n",
    "            yield term, freq\n",
    "\n",
    "d = OrderedDict([(k,v) for k,v in sorted(iter_word_freqs(data), key=operator.itemgetter(1))])\n",
    "ys = [i+0.5 for i,_ in enumerate(d)]\n",
    "plt.barh(ys, d.values(), align='center')\n",
    "plt.yticks(ys, list(d.keys()))\n",
    "plt.axis([0,50,0-0.1,len(d)+0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
